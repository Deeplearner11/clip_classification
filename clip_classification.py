# -*- coding: utf-8 -*-
"""Interacting with CLIP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb
"""

! pip install ftfy regex tqdm
! pip install git+https://github.com/openai/CLIP.git

from google.colab import drive
drive.mount('/content/gdrive')

img_path = "/content/gdrive/MyDrive/ppe/images"

import numpy as np
import torch
from pkg_resources import packaging

print("Torch version:", torch.__version__)

import clip

clip.available_models()

model, preprocess = clip.load("RN50")
model.cpu().eval()
input_resolution = model.visual.input_resolution
context_length = model.context_length
vocab_size = model.vocab_size

print("Model parameters:", f"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}")
print("Input resolution:", input_resolution)
print("Context length:", context_length)
print("Vocab size:", vocab_size)

preprocess

clip.tokenize("Hello World!")

# Commented out IPython magic to ensure Python compatibility.
import os
import skimage
import IPython.display
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

from collections import OrderedDict
import torch

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

# images in skimage to use and their textual descriptions
descriptions = {

   "basement": "a mechanical room inside basement",
   "construction_site": "a construction site",
   "locker_inside": "a locker room with people sitting on chairs",
     "office_meeting": "inside a office with a table and people sitting on chairs",
    "office_table_meeting":"inside an office room with table, chairs, board and people",
   "people_meeting" : "an office workspace with people",
     "site_people": "2 person on a work site with laptop",
     "warehouse_people": "inside a warehouse with people discussing"
}

original_images = []
images = []
texts = []
plt.figure(figsize=(16, 5))

#for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(".png") or filename.endswith(".jpg")]:
   # name = os.path.splitext(filename)[0]
 #   if name not in descriptions:
   #     continue

  #  image = Image.open(os.path.join(skimage.data_dir, filename)).convert("RGB")
for filename in os.listdir(img_path):
   if [filename.endswith(".png") or filename.endswith(".jpg")]: # Check if the file is an image (you may need to adjust this condition based on your file naming)
    name = os.path.splitext(filename)[0]
    if name not in descriptions:
       continue
        # Load the image and append it to the list of original images
    image = Image.open(os.path.join(img_path, filename)).convert("RGB")
    original_images.append(image)
    images.append(preprocess(image))
    texts.append(descriptions[name])
    plt.subplot(2, 4, len(images) )
    plt.imshow(image)
    plt.title(f"{filename}\n{descriptions[name]}")
    plt.xticks([])
    plt.yticks([])

plt.tight_layout()

image_input = torch.tensor(np.stack(images)).cpu()
text_tokens = clip.tokenize(["This is " + desc for desc in texts]).cpu()

with torch.no_grad():
    image_features = model.encode_image(image_input).float()
    text_features = model.encode_text(text_tokens).float()

image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T

count = len(descriptions)

plt.figure(figsize=(20, 14))
plt.imshow(similarity, vmin=0.1, vmax=0.3)
# plt.colorbar()
plt.yticks(range(count), texts, fontsize=18)
plt.xticks([])
for i, image in enumerate(original_images):
    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin="lower")
for x in range(similarity.shape[1]):
    for y in range(similarity.shape[0]):
        plt.text(x, y, f"{similarity[y, x]:.2f}", ha="center", va="center", size=12)

for side in ["left", "top", "right", "bottom"]:
  plt.gca().spines[side].set_visible(False)

plt.xlim([-0.5, count - 0.5])
plt.ylim([count + 0.5, -2])

plt.title("Cosine similarity between text and image features", size=20)

from torchvision.datasets import CIFAR100

cifar100 = CIFAR100(os.path.expanduser("~/.cache"), transform=preprocess, download=True)

text_descriptions = [f"This is a photo of a {label}" for label in cifar100.classes]

text_tokens = clip.tokenize(text_descriptions).cpu()

with torch.no_grad():
    text_features = model.encode_text(text_tokens).float()
    text_features /= text_features.norm(dim=-1, keepdim=True)

text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)
top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)

plt.figure(figsize=(16, 16))

for i, image in enumerate(original_images):
    plt.subplot(4, 4, 2 * i + 1)
    plt.imshow(image)
    plt.axis("off")

    plt.subplot(4, 4, 2 * i + 2)
    y = np.arange(top_probs.shape[-1])
    plt.grid()
    plt.barh(y, top_probs[i])
    plt.gca().invert_yaxis()
    plt.gca().set_axisbelow(True)
    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])
    plt.xlabel("probability")

plt.subplots_adjust(wspace=0.5)
plt.show()

!pip install openai
!pip install langchain

pip install --upgrade langchain

from langchain.llms import GooglePalm
api_key = "AIzaSyDru-BuGHHzgh9Rwx9tLn1j9ZoFaLgDk18"
llm = GooglePalm(google_api_key=api_key, temperature=0.2)

from langchain.prompts import PromptTemplate
prompt_template_name = PromptTemplate(
    input_variables = ['keywords'],
    template = " Based on the keywords predict whether the image represented by the {keywords} is an outdoor scene image and PPE detection is required or indoor scene image and PPE detection is not required."
)

from langchain.chains import LLMChain
chain = LLMChain(llm =llm,prompt = prompt_template_name)
chain.invoke("table,woman,plain,cup,chair")