# clip_classification
The CLIP (Contrastive Language-Image Pre-training) model, developed by OpenAI, is a multi-modal vision and language model. It maps images and text descriptions to the same latent space, allowing it to determine whether an image and description match. 
![Screenshot 2024-04-11 193532](https://github.com/Deeplearner11/clip_classification/assets/87230145/93aa5052-018d-4862-b80b-21ff58effd68)

